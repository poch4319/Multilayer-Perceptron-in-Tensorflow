import numpy as np
import random

from dataclasses import dataclass
from enum import Enum
from typing import Dict, Generator, List, Optional, Tuple


class FeatureNormalizationType(Enum):
    NONE   = "none"
    MINMAX = "minmax"
    STD    = "std"


@dataclass
class Example:
    label: str
    features: List[float]


def read_iris_data(filename: str,
                   normalization_type: str
                  )-> Tuple[List[Example], List[str], List[str]]:
    """ Read the Iris data set from a CSV file. Returns a list of examples
        (which are dictionaries). Optionally applies normalization to features.

              filename: Filename to read from
    normalization_type: Either None, 'minmax' or 'std'
    """
    with open(filename, 'r') as f:
        all_examples = []                                                      # The data set
        feature_map = []                                                       # Maps from feature names to ids (their index in a list)
        label_map = []                                                         # Maps from label names to ids (their index in this list)
        i = 0
        all_features = []                                                      # Used for normalization
        while True:
            line = f.readline()
            if line:
                parts = line.strip().split(',')
                features = parts[:-1]
                label = parts[-1]                                              # The last value is the label
                if i == 0:                                                     # This is the CSV header
                    feature_map  = features                                    # That's the feature map
                    all_features = [np.array([])]*len(features)                # Setup numpy arrays
                else:
                    example = Example(label=label,
                                     features=[float(i) for i in features])    # A training example has a label and features
                    for index in range(len(example.features)):                 # Note that all features are dense, i.e. defined for all examples
                        all_features[index] = np.append(example.features[index],
                                                        all_features[index])
                    all_examples.append(example)
                    if label not in label_map:
                        label_map.append(label)
                i += 1
            else:
                break

        if FeatureNormalizationType(normalization_type) \
            == FeatureNormalizationType.MINMAX:                                #  Min-max normalization, value = (value - min) / (max - min), values in [0, 1]
            minimums = [np.min(v) for v in all_features]
            maximums = [np.max(v) for v in all_features]
            for example in all_examples:
                for index in range(len(example.features)):
                    value = example.features[index]
                    example.features[index] = \
                        (value - minimums[index])  \
                            / (maximums[index] - minimums[index])

        elif FeatureNormalizationType(normalization_type) \
                == FeatureNormalizationType.STD:                               # Standardization, values will have mean 0 and stddev 1
            means = [np.mean(v) for v in all_features]
            stddevs = [np.std(v) for v in all_features]
            for example in all_examples:
                for index in range(len(example.features)):
                    value = example.features[index]
                    example.features[index] = \
                        (value - means[index])  \
                            / stddevs[index]

        return all_examples, feature_map, sorted(label_map)


def one_hot_encoding(value: str,
                     possible_values: List[str]
                    ) -> List[int]:
    """ Encode a single label into a one-hot vector (given a sorted list of all
        possible values).

              value: A single label
    possible_values: All labels, including the given one
    """
    return [1 if i == value else 0 for i in possible_values]


@dataclass
class Batch:
    feature_vectors: List[List[float]]
    label_vectors: List[List[int]]


def batch_generator(data: List[Example],
                    label_map: List[str],
                    batch_size: Optional[int] = 1,
                    randomize: Optional[bool] = True
                   ) -> Generator[Batch, None, None]:
    """ Generate batches of examples, last batch may contain less examples than
        the desired batch size.

          data: The data as generated by read_iris_data()
     label_map: Sorted list of all possible labels
    batch_size: The size of the batches
     randomize: Whether to randomize the data or not
    """
    current_feature_vectors = []
    current_label_vectors = []
    if randomize:
        data = sorted(data, key=lambda x: random.random())
    try:
        c = 0
        while True:
            example = data[c]
            current_feature_vectors.append(example.features)
            encoded_label = one_hot_encoding(example.label, label_map)
            current_label_vectors.append(encoded_label)
            if len(current_feature_vectors) == batch_size:
                yield Batch(feature_vectors=current_feature_vectors,
                            label_vectors=current_label_vectors)
                current_feature_vectors = []
                current_label_vectors = []
            c += 1
    except IndexError:
        if current_feature_vectors:
            yield Batch(feature_vectors=current_feature_vectors,
                        label_vectors=current_label_vectors)


def split_data_set(data: List[Example],
                   ratio: float) -> Tuple[List[Example], List[Example]]:
    """ Split data set into training and testing parts.

    data: The data as generated by read_iris_data()
    ratio: In (0,1]
    """
    n = len(data)
    c = int(n * ratio)
    index = set(range(n))
    a = set(random.sample(index, c))
    b = index - a

    return [data[i] for i in b], [data[i] for i in a]
